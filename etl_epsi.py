# olist_etl_tp.py
import streamlit as st
import os
import base64
import requests
from datetime import datetime, timezone
import pandas as pd

# ---------------------------
# CONFIG
# ---------------------------
st.set_page_config(
    page_title="TP ETL – Olist (e-commerce brésilien)",
    layout="wide"
)

# À adapter si besoin
GITHUB_REPO = "orkhoven/etl_epsi_olist"  # ex: "orkhoven/etl_epsi"
SUBMISSIONS_DIR = "submissions"
GITHUB_TOKEN = st.secrets["GITHUB_TOKEN"]  # même secret que le TP 1

# ---------------------------
# CONSTANTS / HELPERS
# ---------------------------
KAGGLE_URL = "https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce"

REQUIRED_Olist_FILES = [
    "olist_orders_dataset.csv",
    "olist_order_items_dataset.csv",
    "olist_order_payments_dataset.csv",
    "olist_customers_dataset.csv",
]

# Colonnes minimales attendues dans le dataset final
REQUIRED_FINAL_COLUMNS = [
    "order_id",
    "customer_unique_id",
    "order_status",
    "order_purchase_timestamp",
    "order_approved_at",
    "order_delivered_customer_date",
    "order_estimated_delivery_date",
    "nb_items",
    "total_products_value",
    "total_freight_value",
    "total_order_value",
    "nb_payments",
    "total_payment_value",
    "delivery_delay_days",
    "is_late",
]


def funny_llm_detector(text: str) -> bool:
    """
    Détecte grossièrement si le fichier semble généré avec un LLM.
    Juste pour le côté 'blague anti-ChatGPT'.
    """
    markers = [
        "chatgpt", "gpt-4", "gpt-3", "llm", "copilot",
        "generated by", "as an ai language model"
    ]
    lower = text.lower()
    return any(m in lower for m in markers)


def github_api_headers():
    return {
        "Authorization": f"Bearer {GITHUB_TOKEN}",
        "Accept": "application/vnd.github+json"
    }


def github_file_exists(path: str):
    url = f"https://api.github.com/repos/{GITHUB_REPO}/contents/{path}"
    r = requests.get(url, headers=github_api_headers())
    if r.status_code == 200:
        return r.json()["sha"]
    return None


def github_commit_file(path: str, content_bytes: bytes, message: str):
    """
    Crée ou met à jour un fichier dans le repo GitHub.
    """
    url = f"https://api.github.com/repos/{GITHUB_REPO}/contents/{path}"
    existing_sha = github_file_exists(path)

    data = {
        "message": message,
        "content": base64.b64encode(content_bytes).decode("utf-8"),
    }
    if existing_sha is not None:
        data["sha"] = existing_sha

    r = requests.put(url, headers=github_api_headers(), json=data)
    if r.status_code not in (200, 201):
        raise RuntimeError(
            f"Erreur GitHub ({r.status_code}): {r.text}"
        )
    return r.json()


def validate_final_csv(df: pd.DataFrame):
    """
    Vérifications simples sur le résultat ETL.
    Ne corrige rien, juste un feedback aux étudiants.
    """
    errors = []
    warnings = []

    # 1) Colonnes obligatoires
    missing = [c for c in REQUIRED_FINAL_COLUMNS if c not in df.columns]
    if missing:
        errors.append(
            f"Colonnes manquantes dans votre dataset final : {missing}"
        )

    # 2) Types de base (si colonnes présentes)
    datetime_cols = [
        "order_purchase_timestamp",
        "order_approved_at",
        "order_delivered_customer_date",
        "order_estimated_delivery_date",
    ]
    for col in datetime_cols:
        if col in df.columns:
            if not pd.api.types.is_datetime64_any_dtype(df[col]):
                warnings.append(
                    f"La colonne '{col}' n'est pas au format date/heure (datetime)."
                )

    numeric_cols = [
        "nb_items",
        "total_products_value",
        "total_freight_value",
        "total_order_value",
        "nb_payments",
        "total_payment_value",
        "delivery_delay_days",
    ]
    for col in numeric_cols:
        if col in df.columns:
            if not pd.api.types.is_numeric_dtype(df[col]):
                warnings.append(
                    f"La colonne numérique '{col}' n'est pas au bon type (numérique)."
                )

    # 3) Logique simple
    if "is_late" in df.columns:
        unique_vals = set(df["is_late"].dropna().unique().tolist())
        if not unique_vals.issubset({0, 1, True, False}):
            warnings.append(
                "La colonne 'is_late' devrait être un booléen (0/1 ou True/False)."
            )

    return errors, warnings


# ---------------------------
# LAYOUT
# ---------------------------
st.title("TP ETL – Olist (e-commerce brésilien)")

col_intro, col_right = st.columns([2, 1])

with col_intro:
    st.markdown("""
### Contexte métier

Vous travaillez dans l’équipe **Data** d’un acteur e-commerce (Olist) qui vend sur des 
marketplaces au Brésil.

On vous demande de construire un **pipeline ETL** qui part des fichiers bruts 
(Olist sur Kaggle) pour produire un **dataset analytique au niveau de la commande** 
(permettant d’analyser les délais de livraison, le chiffre d’affaires, etc.).

Votre travail doit :

1. **Extraire** les données depuis plusieurs fichiers CSV (Kaggle Olist).
2. **Transformer** les données (nettoyage, jointures, agrégations, calcul de KPIs).
3. **Charger** le résultat final dans un fichier `olist_orders_enrichies.csv` 
   (ou équivalent) prêt pour l’analyse BI / data science.
    """)

with col_right:
    st.info("""
#### Dataset Olist

- **Nom** : Brazilian E-Commerce Public Dataset by Olist  
- **Source** : Kaggle  
- **Lien** :  
  [Ouvrir sur Kaggle](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)

**Fichiers minimum à utiliser :**
- `olist_orders_dataset.csv`  
- `olist_order_items_dataset.csv`  
- `olist_order_payments_dataset.csv`  
- `olist_customers_dataset.csv`
    """)

st.markdown("---")

# ---------------------------
# SECTION 1 – Histoire & objectifs pédagogiques
# ---------------------------
st.header("1. Histoire du TP & objectifs pédagogiques")

st.markdown("""
**Histoire :**

Olist souhaite disposer d’un **tableau de bord** qui permet de répondre à des questions du type :

- Combien de commandes avons-nous livré à temps / en retard ?
- Quels sont les montants moyens des commandes ?
- Comment évolue le chiffre d’affaires par mois ?
- Quels clients commandent le plus souvent ?

Pour cela, il faut construire un **dataset de faits** au niveau de la commande (grain = `order_id`)
à partir des différentes tables brutes.

---

**Objectifs pédagogiques (ETL) :**

- Manipuler un dataset multiparties (plusieurs tables reliées par des clés).
- Mettre en place un **pipeline ETL en Python** avec `pandas`.
- Gérer des **dates**, des **montants**, des **agrégations** et des indicateurs dérivés.
- Produire un **fichier final propre et documenté**.
- Versionner le travail en l’envoyant automatiquement sur GitHub (comme au TP 1).
""")

st.markdown("---")

# ---------------------------
# SECTION 2 – Travail demandé aux étudiants
# ---------------------------
st.header("2. Travail demandé (pipeline ETL à coder en Python)")

st.markdown("""
Vous devez **coder vous-même** le pipeline ETL (dans un notebook ou un fichier `.py`).

### Étape A – Extraction

1. Télécharger le dataset Olist depuis Kaggle.
2. Charger au minimum les CSV suivants avec `pandas.read_csv` :
   - `olist_orders_dataset.csv`
   - `olist_order_items_dataset.csv`
   - `olist_order_payments_dataset.csv`
   - `olist_customers_dataset.csv`

### Étape B – Transformation

À partir de ces tables, vous devez :

1. **Nettoyage & typage**
   - Convertir en `datetime` les colonnes de dates (`order_purchase_timestamp`, 
     `order_approved_at`, `order_delivered_customer_date`, 
     `order_estimated_delivery_date`, etc.).
   - Vérifier les valeurs manquantes importantes et documenter vos choix
     (suppression de lignes, imputation simple…).

2. **Jointures pour obtenir une table au niveau `order_id`**
   - Joindre `orders` avec `customers` (clé : `customer_id`).
   - Joindre `orders` avec `order_items` (clé : `order_id`).
   - Joindre `orders` avec `order_payments` (clé : `order_id`).

3. **Agrégations au niveau de la commande**
   Pour chaque `order_id`, calculer par exemple :
   - `nb_items` : nombre total de lignes dans `order_items` (ou quantité totale).
   - `total_products_value` : somme de `price`.
   - `total_freight_value` : somme de `freight_value`.
   - `total_order_value` = `total_products_value` + `total_freight_value`.
   - `nb_payments` : nombre de lignes dans `order_payments`.
   - `total_payment_value` : somme de `payment_value`.

4. **Indicateurs de délai de livraison**
   - `delivery_delay_days` = 
     `order_delivered_customer_date` − `order_estimated_delivery_date` (en jours).
   - `is_late` :
     - 1 si livraison réelle > date de livraison estimée
     - 0 sinon

5. **Variables temporelles supplémentaires (optionnel mais recommandé)**
   - `order_purchase_date` (date sans heure)
   - `order_purchase_year`
   - `order_purchase_month`
   - etc.

### Étape C – Chargement

Exporter un **dataset final** au format CSV, par exemple :

```text
olist_orders_enrichies.csv
