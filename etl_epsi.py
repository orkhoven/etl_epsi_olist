# olist_etl_tp.py
import streamlit as st
import os
import base64
import requests
from datetime import datetime, timezone
import pandas as pd

# ---------------------------
# CONFIG
# ---------------------------
st.set_page_config(
    page_title="TP ETL – Olist (e-commerce brésilien)",
    layout="wide"
)

# À adapter si besoin
GITHUB_REPO = "orkhoven/etl_epsi_olist"  # ex: "orkhoven/etl_epsi"
SUBMISSIONS_DIR = "submissions"
GITHUB_TOKEN = st.secrets["GITHUB_TOKEN"]  # même secret que le TP 1

# ---------------------------
# CONSTANTS / HELPERS
# ---------------------------
KAGGLE_URL = "https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce"

REQUIRED_Olist_FILES = [
    "olist_orders_dataset.csv",
    "olist_order_items_dataset.csv",
    "olist_order_payments_dataset.csv",
    "olist_customers_dataset.csv",
]

# Colonnes minimales attendues dans le dataset final
REQUIRED_FINAL_COLUMNS = [
    "order_id",
    "customer_unique_id",
    "order_status",
    "order_purchase_timestamp",
    "order_approved_at",
    "order_delivered_customer_date",
    "order_estimated_delivery_date",
    "nb_items",
    "total_products_value",
    "total_freight_value",
    "total_order_value",
    "nb_payments",
    "total_payment_value",
    "delivery_delay_days",
    "is_late",
]


def funny_llm_detector(text: str) -> bool:
    """
    Détecte grossièrement si le fichier semble généré avec un LLM.
    Juste pour le côté 'blague anti-ChatGPT'.
    """
    markers = [
        "chatgpt", "gpt-4", "gpt-3", "llm", "copilot",
        "generated by", "as an ai language model"
    ]
    lower = text.lower()
    return any(m in lower for m in markers)


def github_api_headers():
    return {
        "Authorization": f"Bearer {GITHUB_TOKEN}",
        "Accept": "application/vnd.github+json"
    }


def github_file_exists(path: str):
    url = f"https://api.github.com/repos/{GITHUB_REPO}/contents/{path}"
    r = requests.get(url, headers=github_api_headers())
    if r.status_code == 200:
        return r.json()["sha"]
    return None


def github_commit_file(path: str, content_bytes: bytes, message: str):
    """
    Crée ou met à jour un fichier dans le repo GitHub.
    """
    url = f"https://api.github.com/repos/{GITHUB_REPO}/contents/{path}"
    existing_sha = github_file_exists(path)

    data = {
        "message": message,
        "content": base64.b64encode(content_bytes).decode("utf-8"),
    }
    if existing_sha is not None:
        data["sha"] = existing_sha

    r = requests.put(url, headers=github_api_headers(), json=data)
    if r.status_code not in (200, 201):
        raise RuntimeError(
            f"Erreur GitHub ({r.status_code}): {r.text}"
        )
    return r.json()


def validate_final_csv(df: pd.DataFrame):
    """
    Vérifications simples sur le résultat ETL.
    Ne corrige rien, juste un feedback aux étudiants.
    """
    errors = []
    warnings = []

    # 1) Colonnes obligatoires
    missing = [c for c in REQUIRED_FINAL_COLUMNS if c not in df.columns]
    if missing:
        errors.append(
            f"Colonnes manquantes dans votre dataset final : {missing}"
        )

    # 2) Types de base (si colonnes présentes)
    datetime_cols = [
        "order_purchase_timestamp",
        "order_approved_at",
        "order_delivered_customer_date",
        "order_estimated_delivery_date",
    ]
    for col in datetime_cols:
        if col in df.columns:
            if not pd.api.types.is_datetime64_any_dtype(df[col]):
                warnings.append(
                    f"La colonne '{col}' n'est pas au format date/heure (datetime)."
                )

    numeric_cols = [
        "nb_items",
        "total_products_value",
        "total_freight_value",
        "total_order_value",
        "nb_payments",
        "total_payment_value",
        "delivery_delay_days",
    ]
    for col in numeric_cols:
        if col in df.columns:
            if not pd.api.types.is_numeric_dtype(df[col]):
                warnings.append(
                    f"La colonne numérique '{col}' n'est pas au bon type (numérique)."
                )

    # 3) Logique simple
    if "is_late" in df.columns:
        unique_vals = set(df["is_late"].dropna().unique().tolist())
        if not unique_vals.issubset({0, 1, True, False}):
            warnings.append(
                "La colonne 'is_late' devrait être un booléen (0/1 ou True/False)."
            )

    return errors, warnings


# ---------------------------
# LAYOUT
# ---------------------------
st.title("TP ETL – Olist (e-commerce brésilien)")

col_intro, col_right = st.columns([2, 1])

with col_intro:
    st.markdown(
        "### Contexte métier\n\n"
        "Vous travaillez dans l’équipe **Data** d’un acteur **e-commerce** (Olist) qui vend sur des "
        "marketplaces au Brésil.\n\n"
        "On vous demande de construire un **pipeline ETL** qui part des fichiers bruts "
        "(Olist sur Kaggle) pour produire un **dataset analytique au niveau de la commande** "
        "(permettant d’analyser les délais de livraison, le chiffre d’affaires, etc.).\n\n"
        "Votre travail doit :\n\n"
        "1. **Extraire** les données depuis plusieurs fichiers CSV (Kaggle Olist).\n"
        "2. **Transformer** les données (nettoyage, jointures, agrégations, calcul de KPIs).\n"
        "3. **Charger** le résultat final dans un fichier `olist_orders_enrichies.csv` "
        "(ou équivalent) prêt pour l’analyse BI / data science.\n"
    )

with col_right:
    st.info(
        "#### Dataset Olist\n\n"
        "- **Nom** : Brazilian E-Commerce Public Dataset by Olist  \n"
        "- **Source** : Kaggle  \n"
        "- **Lien** :  \n"
        "  [Ouvrir sur Kaggle](https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce)\n\n"
        "**Fichiers minimum à utiliser :**\n"
        "- `olist_orders_dataset.csv`  \n"
        "- `olist_order_items_dataset.csv`  \n"
        "- `olist_order_payments_dataset.csv`  \n"
        "- `olist_customers_dataset.csv`\n"
    )

st.markdown("---")

# ---------------------------
# SECTION 1 – Histoire & objectifs pédagogiques
# ---------------------------
st.header("1. Histoire du TP & objectifs pédagogiques")

st.markdown(
    "**Histoire :**\n\n"
    "Olist souhaite disposer d’un **tableau de bord** qui permet de répondre à des questions du type :\n\n"
    "- Combien de commandes avons-nous livré à temps / en retard ?\n"
    "- Quels sont les montants moyens des commandes ?\n"
    "- Comment évolue le chiffre d’affaires par mois ?\n"
    "- Quels clients commandent le plus souvent ?\n\n"
    "Pour cela, il faut construire un **dataset de faits** au niveau de la commande (grain = `order_id`) "
    "à partir des différentes tables brutes.\n\n"
    "---\n\n"
    "**Objectifs pédagogiques (ETL) :**\n\n"
    "- Manipuler un dataset multiparties (plusieurs tables reliées par des clés).\n"
    "- Mettre en place un **pipeline ETL en Python** avec `pandas`.\n"
    "- Gérer des **dates**, des **montants**, des **agrégations** et des indicateurs dérivés.\n"
    "- Produire un **fichier final propre et documenté**.\n"
    "- Versionner le travail en l’envoyant automatiquement sur GitHub (comme au TP 1).\n"
)

st.markdown("---")

# ---------------------------
# SECTION 2 – Travail demandé
# ---------------------------
st.header("2. Travail demandé (pipeline ETL à coder en Python)")

st.markdown(
    "Vous devez **coder vous-même** le pipeline ETL (dans un notebook ou un fichier `.py`).\n\n"
    "### Étape A – Extraction\n\n"
    "1. Télécharger le dataset Olist depuis Kaggle.\n"
    "2. Charger au minimum les CSV suivants avec `pandas.read_csv` :\n"
    "   - `olist_orders_dataset.csv`\n"
    "   - `olist_order_items_dataset.csv`\n"
    "   - `olist_order_payments_dataset.csv`\n"
    "   - `olist_customers_dataset.csv`\n\n"
    "### Étape B – Transformation\n\n"
    "À partir de ces tables, vous devez construire **une table finale au niveau de la commande** "
    "(grain = `order_id`).\n\n"
    "Contraintes principales :\n"
    "- La table finale doit contenir les informations de commande, de client, d’articles commandés "
    "  et de paiements.\n"
    "- Vous devez créer des indicateurs de volume (nombre d’articles, montants, etc.).\n"
    "- Vous devez créer au moins un indicateur de délai de livraison et un indicateur de retard.\n"
    "- Vous devez choisir et justifier vos règles de nettoyage / filtrage (dates manquantes, commandes annulées...).\n\n"
    "### Étape C – Chargement\n\n"
    "Exporter un **dataset final** au format CSV, par exemple :\n\n"
    "`olist_orders_enrichies.csv`\n\n"
    "Ce fichier doit **au minimum** contenir des colonnes permettant de :\n"
    "- identifier la commande (`order_id`) et le client (`customer_unique_id`),\n"
    "- connaître le statut de la commande,\n"
    "- analyser les montants (valeur de la commande, frais, paiements...),\n"
    "- analyser les délais et les retards de livraison.\n"
)

# (facultatif) indices dans un expander pour les étudiants en difficulté
with st.expander("Besoin d'un coup de pouce ? (indices, à n'ouvrir qu'en cas de blocage)"):
    st.markdown(
        "- Pensez à convertir les colonnes de dates en `datetime` pour pouvoir calculer des délais.\n"
        "- Les jointures se font autour de `order_id` (et `customer_id` pour les clients).\n"
        "- Un indicateur de retard peut être dérivé de la comparaison entre date réelle de livraison "
        "  et date estimée.\n"
        "- La granularité finale attendue est **une ligne = une commande**.\n"
    )

    "### Étape C – Chargement\n\n"
    "Exporter un **dataset final** au format CSV, par exemple :\n\n"
    "`olist_orders_enrichies.csv`\n\n"
    "Ce fichier doit **au minimum** contenir les colonnes suivantes :\n\n"
    "- `order_id`\n"
    "- `customer_unique_id`\n"
    "- `order_status`\n"
    "- `order_purchase_timestamp`\n"
    "- `order_approved_at`\n"
    "- `order_delivered_customer_date`\n"
    "- `order_estimated_delivery_date`\n"
    "- `nb_items`\n"
    "- `total_products_value`\n"
    "- `total_freight_value`\n"
    "- `total_order_value`\n"
    "- `nb_payments`\n"
    "- `total_payment_value`\n"
    "- `delivery_delay_days`\n"
    "- `is_late`\n"
)

# ---------------------------
# SECTION 3 – Upload + GitHub
# ---------------------------
st.header("3. Dépôt du travail (upload + envoi GitHub)")

st.markdown(
    "Vous allez maintenant **envoyer votre travail** via cette interface.\n\n"
    "- Fichier attendu :  \n"
    "  - soit votre **dataset final** (`olist_orders_enrichies.csv`),  \n"
    "  - soit un fichier `.py` ou un notebook exporté (`.ipynb` converti en `.py`) "
    "    qui contient le pipeline ETL.\n"
    "- Le fichier sera automatiquement déposé dans le dépôt GitHub du cours "
    "  dans le dossier `submissions_olist/`.\n"
)

with st.expander("Rappel : structure minimale attendue si vous envoyez un CSV"):
    st.code("\n".join(REQUIRED_FINAL_COLUMNS), language="text")

with st.form("upload_form"):
    col1, col2 = st.columns(2)

    with col1:
        student_name = st.text_input("Votre nom / prénom (obligatoire)")
        student_email = st.text_input("Votre e-mail (optionnel)")
    with col2:
        group = st.text_input("Groupe / Classe (optionnel)")
        comment = st.text_area("Commentaire (optionnel)")

    uploaded_file = st.file_uploader(
        "Déposez ici votre fichier final (CSV ou .py)",
        type=["csv", "py"],
        help="Exemple : olist_orders_enrichies.csv ou etl_olist.py"
    )

    submitted = st.form_submit_button("Envoyer vers GitHub")

if submitted:
    if not student_name.strip():
        st.error("Merci de renseigner au minimum votre nom / prénom.")
    elif uploaded_file is None:
        st.error("Merci de déposer un fichier avant de soumettre.")
    else:
        # Lecture du fichier uploadé
        file_bytes = uploaded_file.read()
        try:
            file_text = file_bytes.decode("utf-8", errors="ignore")
        except Exception:
            file_text = ""

        # Petit check humour anti-LLM
        if funny_llm_detector(file_text):
            st.error(
                "Détection d'une forte odeur de ChatGPT/Copilot dans ce fichier.\n\n"
                "Merci de **coder vous-même** votre ETL, sinon le prof sera triste."
            )
        else:
            # S'il s'agit d'un CSV, tentative de validation rapide
            if uploaded_file.name.lower().endswith(".csv"):
                import io
                try:
                    df = pd.read_csv(io.BytesIO(file_bytes), low_memory=False)
                    errors, warnings = validate_final_csv(df)
                    if errors:
                        st.error("Problèmes détectés dans votre dataset final :")
                        for e in errors:
                            st.write(f"- {e}")
                    else:
                        if warnings:
                            st.warning("Remarques sur votre dataset :")
                            for w in warnings:
                                st.write(f"- {w}")
                        else:
                            st.success(
                                "Validation basique OK – votre dataset a l'air cohérent "
                                "(au moins structurellement)."
                            )
                except Exception as e:
                    st.warning(
                        f"Impossible de lire le CSV pour validation: {e}. "
                        "Le fichier sera quand même envoyé sur GitHub."
                    )

            # Construction du chemin GitHub
            now = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
            safe_name = student_name.strip().replace(" ", "_")
            base_filename = os.path.splitext(uploaded_file.name)[0]
            ext = os.path.splitext(uploaded_file.name)[1]

            filename = f"{now}__{safe_name}__{base_filename}{ext}"
            github_path = f"{SUBMISSIONS_DIR}/{filename}"

            meta_comment = (
                f"Nom: {student_name} | Email: {student_email} | "
                f"Groupe: {group} | Commentaire: {comment}"
            )
            commit_message = (
                f"TP Olist ETL – dépôt de {student_name} – {now}"
            )

            # Concaténer un petit header texte dans le fichier si c'est du .py
            # (pour garder les métadonnées dans le repo)
            if ext.lower() == ".py" and file_text:
                header = (
                    "# === META ETUDIANT ===\n"
                    f"# {meta_comment}\n"
                    "# =====================\n\n"
                )
                new_text = header + file_text
                file_bytes = new_text.encode("utf-8")

            try:
                github_commit_file(
                    github_path,
                    file_bytes,
                    commit_message
                )
                st.success(
                    f"Fichier envoyé sur GitHub sous : `{github_path}`"
                )
            except Exception as e:
                st.error(
                    f"Erreur lors de l'envoi vers GitHub : {e}"
                )

st.markdown("---")

st.caption(
    "Ce TP correspond à la deuxième partie du cours ETL : "
    "mise en pratique avancée avec un dataset e-commerce multi-tables (Olist)."
)
